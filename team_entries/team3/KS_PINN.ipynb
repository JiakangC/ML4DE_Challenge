{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49bfc51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dab1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Class\n",
    "\n",
    "# Fully connected neural network with number of layers, neurons, input size, output size and activation function as parameters\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_layers, num_neurons, input_size, output_size, activation_function):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(nn.Linear(input_size, num_neurons))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(nn.Linear(num_neurons, num_neurons))\n",
    "\n",
    "        # Output layer\n",
    "        self.layers.append(nn.Linear(num_neurons, output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.activation_function(layer(x))\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "\n",
    "## define the activation functions: tanh() and sin()\n",
    "def tanh(x):\n",
    "    return torch.tanh(x)\n",
    "\n",
    "def sin(x):\n",
    "    return torch.sin(x)\n",
    "\n",
    "# input size = 2, (x, t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7d71e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial condition \n",
    "def ic_func(x):\n",
    "    return np.cos(x[:, 0:1]) + 0.1 * np.sin(2 * x[:, 0:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5cc91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boundary condition\n",
    "\n",
    "## periodic boundary condition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00717d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function construction\n",
    "\n",
    "## Residual loss function\n",
    "\n",
    "def Residual_NN(x, u, t, u_t, u_xx):\n",
    "    # Compute the residual of the PDE\n",
    "    f = u_t + u_xx - np.sin(t) * np.cos(x)\n",
    "\n",
    "    dy_dx = torch.autograd.grad(y, x, torch.ones_like(y), create_graph=True)[0]\n",
    "    return f\n",
    "\n",
    "## Boundary loss function\n",
    "def Boundary_NN(x, t, u):\n",
    "    # Compute the boundary condition loss\n",
    "    u_bc = torch.zeros_like(u)\n",
    "    return u - u_bc\n",
    "\n",
    "## Initial loss function\n",
    "def Initial_NN(x, u):\n",
    "    # Compute the initial condition loss\n",
    "    u_ic = ic_func(x)\n",
    "    return u - u_ic\n",
    "\n",
    "## training loss function\n",
    "def train_loss(data, model):\n",
    "    x = x.requires_grad_()\n",
    "    t = t.requires_grad_()\n",
    "    u = u.requires_grad_()\n",
    "    u_t = u_t.requires_grad_()\n",
    "    u_xx = u_xx.requires_grad_()\n",
    "\n",
    "\n",
    "    # Compute the total loss\n",
    "    res_loss = Residual_NN(x, u, t, u_t, u_xx)\n",
    "    bc_loss = Boundary_NN(x, t, u)\n",
    "    ic_loss = Initial_NN(x, u)\n",
    "    total_loss = res_loss + bc_loss + ic_loss\n",
    "    return total_loss, res_loss,  ic_loss\n",
    "\n",
    "import torch\n",
    "\n",
    "# === Helper: Compute Derivatives for KS Equation ===\n",
    "def compute_ks_derivatives(model, x, t):\n",
    "    \"\"\"\n",
    "    Given a model that predicts u(x,t), compute:\n",
    "      u, u_t, u_x, u_xx, u_xxx, and u_xxxx \n",
    "    using automatic differentiation.\n",
    "\n",
    "    Inputs:\n",
    "      model : the neural network model, taking (x, t)\n",
    "      x, t  : tensors with requires_grad=True\n",
    "  \n",
    "    Returns:\n",
    "      u, u_t, u_x, u_xx, u_xxx, u_xxxx\n",
    "    \"\"\"\n",
    "    # Make sure x and t require gradients\n",
    "    x = x.requires_grad_()\n",
    "    t = t.requires_grad_()\n",
    "\n",
    "    # Forward pass: predict u\n",
    "    u = model(x, t)\n",
    "\n",
    "    # First derivative with respect to time: u_t\n",
    "    u_t = torch.autograd.grad(\n",
    "        u, t,\n",
    "        grad_outputs=torch.ones_like(u),\n",
    "        retain_graph=True,\n",
    "        create_graph=True\n",
    "    )[0]\n",
    "\n",
    "    # First derivative with respect to space: u_x\n",
    "    u_x = torch.autograd.grad(\n",
    "        u, x,\n",
    "        grad_outputs=torch.ones_like(u),\n",
    "        retain_graph=True,\n",
    "        create_graph=True\n",
    "    )[0]\n",
    "\n",
    "    # Second derivative with respect to space: u_xx\n",
    "    u_xx = torch.autograd.grad(\n",
    "        u_x, x,\n",
    "        grad_outputs=torch.ones_like(u_x),\n",
    "        retain_graph=True,\n",
    "        create_graph=True\n",
    "    )[0]\n",
    "\n",
    "    # Third derivative with respect to space: u_xxx\n",
    "    u_xxx = torch.autograd.grad(\n",
    "        u_xx, x,\n",
    "        grad_outputs=torch.ones_like(u_xx),\n",
    "        retain_graph=True,\n",
    "        create_graph=True\n",
    "    )[0]\n",
    "\n",
    "    # Fourth derivative with respect to space: u_xxxx\n",
    "    u_xxxx = torch.autograd.grad(\n",
    "        u_xxx, x,\n",
    "        grad_outputs=torch.ones_like(u_xxx),\n",
    "        retain_graph=True,\n",
    "        create_graph=True\n",
    "    )[0]\n",
    "\n",
    "    return u, u_t, u_x, u_xx, u_xxx, u_xxxx\n",
    "\n",
    "\n",
    "# === Residual Loss Function for KS Equation ===\n",
    "def residual_loss(model, x_f, t_f, nu):\n",
    "    \"\"\"\n",
    "    Computes the residual loss for the KS equation:\n",
    "       u_t + u*u_x + u_xx + nu*u_xxxx = 0\n",
    "  \n",
    "    Inputs:\n",
    "      model: neural network that predicts u(x,t)\n",
    "      x_f, t_f: collocation (interior) points where the PDE is enforced\n",
    "      nu: parameter nu in the equation\n",
    "\n",
    "    Returns:\n",
    "      loss_res: a scalar tensor representing the Mean Squared Error (MSE)\n",
    "                of the PDE residual.\n",
    "    \"\"\"\n",
    "    # Compute u and necessary derivatives at collocation points:\n",
    "    u, u_t, u_x, u_xx, _, u_xxxx = compute_ks_derivatives(model, x_f, t_f)\n",
    "    \n",
    "    # Compute the PDE residual f:\n",
    "    # f = u_t + u*u_x + u_xx + nu*u_xxxx\n",
    "    f = u_t + u * u_x + u_xx + nu * u_xxxx\n",
    "\n",
    "    # Mean Squared Error of the residual (forcing f=0):\n",
    "    loss_res = torch.mean(f**2)\n",
    "    return loss_res\n",
    "\n",
    "\n",
    "# === Boundary Loss Function for Periodic BC ===\n",
    "def boundary_loss(model, x_left, t_left, x_right, t_right):\n",
    "    \"\"\"\n",
    "    Computes the boundary loss for enforcing periodic boundary conditions.\n",
    "    For a periodic domain [a, b] we enforce u(a,t) = u(b,t).\n",
    "\n",
    "    Inputs:\n",
    "      model: neural network that predicts u(x,t)\n",
    "      x_left, t_left: points on the left boundary (e.g., x=a)\n",
    "      x_right, t_right: corresponding points on the right boundary (e.g., x=b)\n",
    "\n",
    "    Returns:\n",
    "      loss_bc: the Mean Squared Error (MSE) between u(x_left,t_left) and u(x_right,t_right).\n",
    "    \"\"\"\n",
    "    u_left = model(x_left, t_left)\n",
    "    u_right = model(x_right, t_right)\n",
    "\n",
    "    loss_bc = torch.mean((u_left - u_right)**2)\n",
    "    return loss_bc\n",
    "\n",
    "\n",
    "# === Initial Loss Function ===\n",
    "def initial_loss(model, x_ic, t_ic, u_ic_target):\n",
    "    \"\"\"\n",
    "    Computes the loss for the initial condition:\n",
    "        u(x, t=0) = u_ic_target(x)\n",
    "\n",
    "    Inputs:\n",
    "      model: neural network that predicts u(x,t)\n",
    "      x_ic, t_ic: the initial condition points (typically t_ic is zero)\n",
    "      u_ic_target: true initial values obtained from your ic function\n",
    "\n",
    "    Returns:\n",
    "      loss_ic: Mean Squared Error (MSE) between network prediction and initial condition target.\n",
    "    \"\"\"\n",
    "    u_ic_pred = model(x_ic, t_ic)\n",
    "    loss_ic = torch.mean((u_ic_pred - u_ic_target)**2)\n",
    "    return loss_ic\n",
    "\n",
    "\n",
    "# === Combined Training Loss Function ===\n",
    "def train_loss(model, data, nu):\n",
    "    \"\"\"\n",
    "    Computes the total loss by combining:\n",
    "      - the residual loss (enforcing the KS PDE)\n",
    "      - the boundary loss (enforcing periodic BC)\n",
    "      - the initial loss (enforcing the IC)\n",
    "      \n",
    "    The `data` dictionary is assumed to have the following keys:\n",
    "      - 'collocation': tuple (x_f, t_f)\n",
    "      - 'boundary': tuple (x_left, t_left, x_right, t_right)\n",
    "      - 'initial': tuple (x_ic, t_ic, u_ic_target)\n",
    "      \n",
    "    nu is the KS equation parameter.\n",
    "\n",
    "    Returns:\n",
    "      total_loss: the sum of the three loss terms.\n",
    "      loss_r, loss_b, loss_i: individual loss components.\n",
    "    \"\"\"\n",
    "    # Unpack the data dictionary\n",
    "    x_f, t_f = data['collocation']\n",
    "    x_left, t_left, x_right, t_right = data['boundary']\n",
    "    x_ic, t_ic, u_ic_target = data['initial']\n",
    "\n",
    "    # Compute each loss term:\n",
    "    loss_r = residual_loss(model, x_f, t_f, nu)\n",
    "    loss_b = boundary_loss(model, x_left, t_left, x_right, t_right)\n",
    "    loss_i = initial_loss(model, x_ic, t_ic, u_ic_target)\n",
    "\n",
    "    # Combine the losses (you can also weight these if needed)\n",
    "    total_loss = loss_r + loss_b + loss_i\n",
    "    return total_loss, loss_r, loss_b, loss_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df9dd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training process\n",
    "epochs = 5000\n",
    "\n",
    "loss_history = []\n",
    "ode_loss_history = []\n",
    "initial_loss_history = []\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "model = NeuralNetwork(num_layers=4, num_neurons=50, input_size=1, output_size=1, activation_function=tanh)\n",
    "print(model)\n",
    "\n",
    "x_train = torch.linspace(-2, 2, 100).view(-1, 1)  # Training points\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    total_loss, ode_loss, initial_loss = train_loss(model, x_train)\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    \n",
    "    loss_history.append(total_loss.item())\n",
    "    ode_loss_history.append(ode_loss.item())\n",
    "    initial_loss_history.append(initial_loss.item())\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss.item():.6f}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML4DE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
