{
 "cells": [
  {
   "cell_type": "code",
   "id": "49bfc51d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:02:13.902502Z",
     "start_time": "2025-04-09T15:02:11.485535Z"
    }
   },
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dab1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Class\n",
    "\n",
    "# Fully connected neural network with number of layers, neurons, input size, output size and activation function as parameters\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_layers, num_neurons, input_size, output_size, activation_function):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "        # Input layer\n",
    "        self.layers.append(nn.Linear(input_size, num_neurons))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(nn.Linear(num_neurons, num_neurons))\n",
    "\n",
    "        # Output layer\n",
    "        self.layers.append(nn.Linear(num_neurons, output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.activation_function(layer(x))\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "\n",
    "## define the activation functions: tanh() and sin()\n",
    "def tanh(x):\n",
    "    return torch.tanh(x)\n",
    "\n",
    "def sin(x):\n",
    "    return torch.sin(x)\n",
    "\n",
    "# input size = 2, (x, t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7d71e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial condition \n",
    "def ic_func(x):\n",
    "    return np.cos(x[:, 0:1]) + 0.1 * np.sin(2 * x[:, 0:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5cc91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boundary condition\n",
    "\n",
    "## periodic boundary condition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00717d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function construction\n",
    "\n",
    "## Residual loss function\n",
    "\n",
    "def Residual_NN(x, u, t, u_t, u_xx):\n",
    "    # Compute the residual of the PDE\n",
    "    f = u_t + u_xx - np.sin(t) * np.cos(x)\n",
    "\n",
    "    dy_dx = torch.autograd.grad(y, x, torch.ones_like(y), create_graph=True)[0]\n",
    "    return f\n",
    "\n",
    "## Boundary loss function\n",
    "def Boundary_NN(x, t, u):\n",
    "    # Compute the boundary condition loss\n",
    "    u_bc = torch.zeros_like(u)\n",
    "    return u - u_bc\n",
    "\n",
    "## Initial loss function\n",
    "def Initial_NN(x, u):\n",
    "    # Compute the initial condition loss\n",
    "    u_ic = ic_func(x)\n",
    "    return u - u_ic\n",
    "\n",
    "## training loss function\n",
    "def train_loss(data, model):\n",
    "    x = x.requires_grad_()\n",
    "    t = t.requires_grad_()\n",
    "    u = u.requires_grad_()\n",
    "    u_t = u_t.requires_grad_()\n",
    "    u_xx = u_xx.requires_grad_()\n",
    "\n",
    "\n",
    "    # Compute the total loss\n",
    "    res_loss = Residual_NN(x, u, t, u_t, u_xx)\n",
    "    bc_loss = Boundary_NN(x, t, u)\n",
    "    ic_loss = Initial_NN(x, u)\n",
    "    total_loss = res_loss + bc_loss + ic_loss\n",
    "    return total_loss, res_loss,  ic_loss\n",
    "\n",
    "import torch\n",
    "\n",
    "# === Helper: Compute Derivatives for KS Equation ===\n",
    "def compute_ks_derivatives(model, x, t):\n",
    "    \"\"\"\n",
    "    Given a model that predicts u(x,t), compute:\n",
    "      u, u_t, u_x, u_xx, u_xxx, and u_xxxx \n",
    "    using automatic differentiation.\n",
    "\n",
    "    Inputs:\n",
    "      model : the neural network model, taking (x, t)\n",
    "      x, t  : tensors with requires_grad=True\n",
    "  \n",
    "    Returns:\n",
    "      u, u_t, u_x, u_xx, u_xxx, u_xxxx\n",
    "    \"\"\"\n",
    "    # Make sure x and t require gradients\n",
    "    x = x.requires_grad_()\n",
    "    t = t.requires_grad_()\n",
    "\n",
    "    # Forward pass: predict u\n",
    "    u = model(x, t)\n",
    "\n",
    "    # First derivative with respect to time: u_t\n",
    "    u_t = torch.autograd.grad(\n",
    "        u, t,\n",
    "        grad_outputs=torch.ones_like(u),\n",
    "        retain_graph=True,\n",
    "        create_graph=True\n",
    "    )[0]\n",
    "\n",
    "    # First derivative with respect to space: u_x\n",
    "    u_x = torch.autograd.grad(\n",
    "        u, x,\n",
    "        grad_outputs=torch.ones_like(u),\n",
    "        retain_graph=True,\n",
    "        create_graph=True\n",
    "    )[0]\n",
    "\n",
    "    # Second derivative with respect to space: u_xx\n",
    "    u_xx = torch.autograd.grad(\n",
    "        u_x, x,\n",
    "        grad_outputs=torch.ones_like(u_x),\n",
    "        retain_graph=True,\n",
    "        create_graph=True\n",
    "    )[0]\n",
    "\n",
    "    # Third derivative with respect to space: u_xxx\n",
    "    u_xxx = torch.autograd.grad(\n",
    "        u_xx, x,\n",
    "        grad_outputs=torch.ones_like(u_xx),\n",
    "        retain_graph=True,\n",
    "        create_graph=True\n",
    "    )[0]\n",
    "\n",
    "    # Fourth derivative with respect to space: u_xxxx\n",
    "    u_xxxx = torch.autograd.grad(\n",
    "        u_xxx, x,\n",
    "        grad_outputs=torch.ones_like(u_xxx),\n",
    "        retain_graph=True,\n",
    "        create_graph=True\n",
    "    )[0]\n",
    "\n",
    "    return u, u_t, u_x, u_xx, u_xxx, u_xxxx\n",
    "\n",
    "\n",
    "# === Residual Loss Function for KS Equation ===\n",
    "def residual_loss(model, x_f, t_f, nu):\n",
    "    \"\"\"\n",
    "    Computes the residual loss for the KS equation:\n",
    "       u_t + u*u_x + u_xx + nu*u_xxxx = 0\n",
    "  \n",
    "    Inputs:\n",
    "      model: neural network that predicts u(x,t)\n",
    "      x_f, t_f: collocation (interior) points where the PDE is enforced\n",
    "      nu: parameter nu in the equation\n",
    "\n",
    "    Returns:\n",
    "      loss_res: a scalar tensor representing the Mean Squared Error (MSE)\n",
    "                of the PDE residual.\n",
    "    \"\"\"\n",
    "    # Compute u and necessary derivatives at collocation points:\n",
    "    u, u_t, u_x, u_xx, _, u_xxxx = compute_ks_derivatives(model, x_f, t_f)\n",
    "    \n",
    "    # Compute the PDE residual f:\n",
    "    # f = u_t + u*u_x + u_xx + nu*u_xxxx\n",
    "    f = u_t + u * u_x + u_xx + nu * u_xxxx\n",
    "\n",
    "    # Mean Squared Error of the residual (forcing f=0):\n",
    "    loss_res = torch.mean(f**2)\n",
    "    return loss_res\n",
    "\n",
    "\n",
    "# === Boundary Loss Function for Periodic BC ===\n",
    "def boundary_loss(model, x_left, t_left, x_right, t_right):\n",
    "    \"\"\"\n",
    "    Computes the boundary loss for enforcing periodic boundary conditions.\n",
    "    For a periodic domain [a, b] we enforce u(a,t) = u(b,t).\n",
    "\n",
    "    Inputs:\n",
    "      model: neural network that predicts u(x,t)\n",
    "      x_left, t_left: points on the left boundary (e.g., x=a)\n",
    "      x_right, t_right: corresponding points on the right boundary (e.g., x=b)\n",
    "\n",
    "    Returns:\n",
    "      loss_bc: the Mean Squared Error (MSE) between u(x_left,t_left) and u(x_right,t_right).\n",
    "    \"\"\"\n",
    "    u_left = model(x_left, t_left)\n",
    "    u_right = model(x_right, t_right)\n",
    "\n",
    "    loss_bc = torch.mean((u_left - u_right)**2)\n",
    "    return loss_bc\n",
    "\n",
    "\n",
    "# === Initial Loss Function ===\n",
    "def initial_loss(model, x_ic, t_ic, u_ic_target):\n",
    "    \"\"\"\n",
    "    Computes the loss for the initial condition:\n",
    "        u(x, t=0) = u_ic_target(x)\n",
    "\n",
    "    Inputs:\n",
    "      model: neural network that predicts u(x,t)\n",
    "      x_ic, t_ic: the initial condition points (typically t_ic is zero)\n",
    "      u_ic_target: true initial values obtained from your ic function\n",
    "\n",
    "    Returns:\n",
    "      loss_ic: Mean Squared Error (MSE) between network prediction and initial condition target.\n",
    "    \"\"\"\n",
    "    u_ic_pred = model(x_ic, t_ic)\n",
    "    loss_ic = torch.mean((u_ic_pred - u_ic_target)**2)\n",
    "    return loss_ic\n",
    "\n",
    "\n",
    "# === Combined Training Loss Function ===\n",
    "def train_loss(model, data, nu):\n",
    "    \"\"\"\n",
    "    Computes the total loss by combining:\n",
    "      - the residual loss (enforcing the KS PDE)\n",
    "      - the boundary loss (enforcing periodic BC)\n",
    "      - the initial loss (enforcing the IC)\n",
    "      \n",
    "    The `data` dictionary is assumed to have the following keys:\n",
    "      - 'collocation': tuple (x_f, t_f)\n",
    "      - 'boundary': tuple (x_left, t_left, x_right, t_right)\n",
    "      - 'initial': tuple (x_ic, t_ic, u_ic_target)\n",
    "      \n",
    "    nu is the KS equation parameter.\n",
    "\n",
    "    Returns:\n",
    "      total_loss: the sum of the three loss terms.\n",
    "      loss_r, loss_b, loss_i: individual loss components.\n",
    "    \"\"\"\n",
    "    # Unpack the data dictionary\n",
    "    x_f, t_f = data['collocation']\n",
    "    x_left, t_left, x_right, t_right = data['boundary']\n",
    "    x_ic, t_ic, u_ic_target = data['initial']\n",
    "\n",
    "    # Compute each loss term:\n",
    "    loss_r = residual_loss(model, x_f, t_f, nu)\n",
    "    loss_b = boundary_loss(model, x_left, t_left, x_right, t_right)\n",
    "    loss_i = initial_loss(model, x_ic, t_ic, u_ic_target)\n",
    "\n",
    "    # Combine the losses (you can also weight these if needed)\n",
    "    total_loss = loss_r + loss_b + loss_i\n",
    "    return total_loss, loss_r, loss_b, loss_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df9dd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training process\n",
    "epochs = 5000\n",
    "\n",
    "loss_history = []\n",
    "ode_loss_history = []\n",
    "initial_loss_history = []\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "model = NeuralNetwork(num_layers=4, num_neurons=50, input_size=1, output_size=1, activation_function=tanh)\n",
    "print(model)\n",
    "\n",
    "x_train = torch.linspace(-2, 2, 100).view(-1, 1)  # Training points\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    total_loss, ode_loss, initial_loss = train_loss(model, x_train)\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    \n",
    "    loss_history.append(total_loss.item())\n",
    "    ode_loss_history.append(ode_loss.item())\n",
    "    initial_loss_history.append(initial_loss.item())\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss.item():.6f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4f08ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def load_training_data(params):\n",
    "    \"\"\"\n",
    "    Load and process training data for the KS equation.\n",
    "    \n",
    "    The training data is expected to be stored in a .npy file with shape:\n",
    "      (num_time_points, num_space_points)\n",
    "    which represents the solution u(x,t) on a grid. This function creates\n",
    "    observation points (x, t) corresponding to the values in the data.\n",
    "    \n",
    "    Parameters:\n",
    "      params (dict): A dictionary containing:\n",
    "          'T'  : Total time domain length (we use first T/2 for training).\n",
    "          'L'  : Spatial domain length.\n",
    "          'N'  : Expected number of spatial points.\n",
    "    \n",
    "    Returns:\n",
    "      data_dict (dict): A dictionary with keys:\n",
    "            'x_data'      : Tensor of x coordinates (shape [N_obs, 1]).\n",
    "            't_data'      : Tensor of t coordinates (shape [N_obs, 1]).\n",
    "            'u_data_target': Tensor of u values (observations, shape [N_obs, 1]).\n",
    "    \"\"\"\n",
    "    # Load the training data\n",
    "    training_data = np.load('../../../data/ks_training.npy')\n",
    "    \n",
    "    # Get dimensions from training data\n",
    "    n_time, n_space = training_data.shape\n",
    "    \n",
    "    # Check for consistency with parameters (if provided)\n",
    "    if 'N' in params and params['N'] != n_space:\n",
    "        print(\"Warning: params['N'] (%d) does not match the number of spatial points in the training data (%d). Using data shape.\" \n",
    "              % (params['N'], n_space))\n",
    "    \n",
    "    # Create the time axis for the first half of the time domain\n",
    "    t = np.linspace(0, params['T'] / 2, n_time)\n",
    "    # Create the spatial axis from 0 to L\n",
    "    x = np.linspace(0, params['L'], n_space)\n",
    "    \n",
    "    # Create an observation grid via meshgrid.\n",
    "    # X holds spatial coordinates and T_grid holds time coordinates.\n",
    "    X, T_grid = np.meshgrid(x, t)\n",
    "    \n",
    "    # Flatten the grid to generate observation points.\n",
    "    # Each observation point has the structure [x, t].\n",
    "    X_train = np.hstack((X.flatten()[:, None], T_grid.flatten()[:, None]))\n",
    "    \n",
    "    # Flatten the training data to get corresponding u values.\n",
    "    y_train = training_data.flatten()[:, None]\n",
    "    \n",
    "    # Convert the data to PyTorch tensors.\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "    \n",
    "    # Split the input into x_data and t_data.\n",
    "    x_data = X_train[:, 0:1]\n",
    "    t_data = X_train[:, 1:2]\n",
    "    \n",
    "    # Pack the tensors in a structured dictionary.\n",
    "    data_dict = {\n",
    "        'x_data': x_data,\n",
    "        't_data': t_data,\n",
    "        'u_data_target': y_train\n",
    "    }\n",
    "    \n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "161c69b9",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-04-09T15:35:26.359708Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# ===================================================\n",
    "# 1. Activation Functions\n",
    "# ===================================================\n",
    "\n",
    "def tanh(x):\n",
    "    return torch.tanh(x)\n",
    "\n",
    "def sin(x):\n",
    "    return torch.sin(x)\n",
    "\n",
    "# ===================================================\n",
    "# 2. Neural Network Definition for (x, t) Inputs\n",
    "# ===================================================\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_layers, num_neurons, input_size, output_size, activation_function):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "        # Input layer (input_size should be 2 for x and t)\n",
    "        self.layers.append(nn.Linear(input_size, num_neurons))\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(nn.Linear(num_neurons, num_neurons))\n",
    "        # Output layer\n",
    "        self.layers.append(nn.Linear(num_neurons, output_size))\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Concatenate spatial and temporal inputs along the feature dimension.\n",
    "        inp = torch.cat((x, t), dim=1)\n",
    "        for layer in self.layers[:-1]:\n",
    "            inp = self.activation_function(layer(inp))\n",
    "        output = self.layers[-1](inp)\n",
    "        return output\n",
    "\n",
    "# ===================================================\n",
    "# 3. Helper: Compute Derivatives for KS Equation\n",
    "# ===================================================\n",
    "\n",
    "def compute_ks_derivatives(model, x, t):\n",
    "    \"\"\"\n",
    "    Computes u, u_t, u_x, u_xx, u_xxx, and u_xxxx using automatic differentiation.\n",
    "    \"\"\"\n",
    "    x = x.requires_grad_()\n",
    "    t = t.requires_grad_()\n",
    "\n",
    "    u = model(x, t)\n",
    "\n",
    "    # Time derivative u_t = du/dt\n",
    "    u_t = torch.autograd.grad(\n",
    "        u, t,\n",
    "        grad_outputs=torch.ones_like(u),\n",
    "        retain_graph=True,\n",
    "        create_graph=True\n",
    "    )[0]\n",
    "\n",
    "    # First spatial derivative u_x = du/dx\n",
    "    u_x = torch.autograd.grad(\n",
    "        u, x,\n",
    "        grad_outputs=torch.ones_like(u),\n",
    "        retain_graph=True,\n",
    "        create_graph=True\n",
    "    )[0]\n",
    "\n",
    "    # Second derivative u_xx = d²u/dx²\n",
    "    u_xx = torch.autograd.grad(\n",
    "        u_x, x,\n",
    "        grad_outputs=torch.ones_like(u_x),\n",
    "        retain_graph=True,\n",
    "        create_graph=True\n",
    "    )[0]\n",
    "\n",
    "    # Third derivative u_xxx = d³u/dx³\n",
    "    u_xxx = torch.autograd.grad(\n",
    "        u_xx, x,\n",
    "        grad_outputs=torch.ones_like(u_xx),\n",
    "        retain_graph=True,\n",
    "        create_graph=True\n",
    "    )[0]\n",
    "\n",
    "    # Fourth derivative u_xxxx = d⁴u/dx⁴\n",
    "    u_xxxx = torch.autograd.grad(\n",
    "        u_xxx, x,\n",
    "        grad_outputs=torch.ones_like(u_xxx),\n",
    "        retain_graph=True,\n",
    "        create_graph=True\n",
    "    )[0]\n",
    "\n",
    "    return u, u_t, u_x, u_xx, u_xxx, u_xxxx\n",
    "\n",
    "# ===================================================\n",
    "# 4. Loss Functions for the KS Equation\n",
    "# ===================================================\n",
    "\n",
    "def residual_loss(model, x_f, t_f, nu):\n",
    "    \"\"\"\n",
    "    Enforce the KS PDE:\n",
    "      u_t + u*u_x + u_xx + nu*u_xxxx = 0.\n",
    "    \"\"\"\n",
    "    u, u_t, u_x, u_xx, _, u_xxxx = compute_ks_derivatives(model, x_f, t_f)\n",
    "    f = u_t + u * u_x + nu * u_xx + nu * u_xxxx\n",
    "    loss_r = torch.mean(f**2)\n",
    "    return loss_r\n",
    "\n",
    "def boundary_loss(model, x_left, t_left, x_right, t_right):\n",
    "    \"\"\"\n",
    "    Enforce periodic boundary conditions:\n",
    "      u(-2, t) = u(2, t).\n",
    "    \"\"\"\n",
    "    u_left = model(x_left, t_left)\n",
    "    u_right = model(x_right, t_right)\n",
    "    loss_b = torch.mean((u_left - u_right)**2)\n",
    "    return loss_b\n",
    "\n",
    "def initial_loss(model, x_ic, t_ic, u_ic_target):\n",
    "    \"\"\"\n",
    "    Enforce the initial condition:\n",
    "      u(x, 0) = u_ic_target(x).\n",
    "    \"\"\"\n",
    "    u_ic_pred = model(x_ic, t_ic)\n",
    "    loss_ic = torch.mean((u_ic_pred - u_ic_target)**2)\n",
    "    return loss_ic\n",
    "\n",
    "def data_loss(model, x_data, t_data, u_data_target):\n",
    "    \"\"\"\n",
    "    Data loss: mean squared error between the network prediction and observed data.\n",
    "    \"\"\"\n",
    "    u_pred = model(x_data, t_data)\n",
    "    loss_d = torch.mean((u_pred - u_data_target)**2)\n",
    "    return loss_d\n",
    "\n",
    "def train_loss(model, data, nu):\n",
    "    \"\"\"\n",
    "    Total loss is the sum of the residual, boundary, initial, and data losses.\n",
    "    \n",
    "    The provided data dictionary should have the following keys:\n",
    "      - 'collocation': tuple (x_f, t_f)\n",
    "      - 'boundary': tuple (x_left, t_left, x_right, t_right)\n",
    "      - 'initial': tuple (x_ic, t_ic, u_ic_target)\n",
    "      - 'data': tuple (x_data, t_data, u_data_target)\n",
    "    \"\"\"\n",
    "    x_f, t_f = data['collocation']\n",
    "    x_left, t_left, x_right, t_right = data['boundary']\n",
    "    x_ic, t_ic, u_ic_target = data['initial']\n",
    "    x_data, t_data, u_data_target = data['data']\n",
    "\n",
    "    loss_r = residual_loss(model, x_f, t_f, nu)\n",
    "    loss_b = boundary_loss(model, x_left, t_left, x_right, t_right)\n",
    "    loss_ic = initial_loss(model, x_ic, t_ic, u_ic_target)\n",
    "    loss_d = data_loss(model, x_data, t_data, u_data_target)\n",
    "\n",
    "    total_loss = loss_r + loss_b + loss_ic + loss_d\n",
    "    return total_loss, loss_r, loss_b, loss_ic, loss_d\n",
    "\n",
    "# ===================================================\n",
    "# 5. Data Loading Functions\n",
    "# ===================================================\n",
    "\n",
    "def load_training_data(params):\n",
    "    \"\"\"\n",
    "    Load and process training data for the KS equation.\n",
    "    \n",
    "    The training data is assumed to be stored in a .npy file with shape:\n",
    "      (num_time_points, num_space_points)\n",
    "    which represents u(x, t) on a grid. This function uses the first half\n",
    "    of the time domain and returns the data in a structured dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "      params (dict): Must include:\n",
    "          'T'  : Total time domain length.\n",
    "          'L'  : Spatial domain length.\n",
    "          'N'  : Expected number of spatial points.\n",
    "          'data_path': Path to the .npy file.\n",
    "    \n",
    "    Returns:\n",
    "      data_dict (dict): Dictionary with keys:\n",
    "            'x_data'       : Tensor of x coordinates (shape [N_obs, 1]).\n",
    "            't_data'       : Tensor of t coordinates (shape [N_obs, 1]).\n",
    "            'u_data_target': Tensor of u observations (shape [N_obs, 1]).\n",
    "    \"\"\"\n",
    "    training_data = np.load(params['data_path'])\n",
    "    n_time, n_space = training_data.shape\n",
    "\n",
    "    if 'N' in params and params['N'] != n_space:\n",
    "        print(\"Warning: params['N'] (%d) does not match the number of spatial points in the training data (%d). Using data shape.\"\n",
    "              % (params['N'], n_space))\n",
    "    \n",
    "    # Use the first half of the time domain\n",
    "    t = np.linspace(0, params['T'] / 2, n_time)\n",
    "    x = np.linspace(0, params['L'], n_space)\n",
    "    \n",
    "    # Create observation grid via meshgrid.\n",
    "    X, T_grid = np.meshgrid(x, t)\n",
    "    X_train = np.hstack((X.flatten()[:, None], T_grid.flatten()[:, None]))\n",
    "    y_train = training_data.flatten()[:, None]\n",
    "    \n",
    "    # Convert to PyTorch tensors.\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "    \n",
    "    # Split input into spatial and temporal components.\n",
    "    x_data = X_train[:, 0:1]\n",
    "    t_data = X_train[:, 1:2]\n",
    "\n",
    "    data_dict = {\n",
    "        'x_data': x_data,\n",
    "        't_data': t_data,\n",
    "        'u_data_target': y_train\n",
    "    }\n",
    "    \n",
    "    return data_dict\n",
    "\n",
    "# ===================================================\n",
    "# 6. Define Initial Condition Function\n",
    "# ===================================================\n",
    "\n",
    "def ic_func(x):\n",
    "    # For example, initial condition u(x,0) = cos(x)\n",
    "    return torch.cos(x) + 0.1 * torch.sin(2 * x)\n",
    "\n",
    "# ===================================================\n",
    "# 7. Generate Other Training Data Components\n",
    "# ===================================================\n",
    "\n",
    "# Domain settings\n",
    "x_min, x_max = 0.0 , 1.0\n",
    "t_min, t_max = 0.0, 100\n",
    "\n",
    "# Collocation points for the PDE residual\n",
    "N_f = 1000\n",
    "x_f = torch.rand(N_f, 1)*(x_max-x_min)+x_min\n",
    "t_f = torch.rand(N_f, 1) * (t_max - t_min) + t_min\n",
    "\n",
    "# Boundary points for enforcing periodic conditions: u(x_min, t)=u(x_max, t)\n",
    "N_b = 100\n",
    "x_left = x_min * torch.ones(N_b, 1)\n",
    "x_right = x_max * torch.ones(N_b, 1)\n",
    "t_b = torch.rand(N_b, 1) * (t_max - t_min) + t_min\n",
    "t_left = t_b\n",
    "t_right = t_b\n",
    "\n",
    "# Initial condition points (t = 0)\n",
    "N_ic = 100\n",
    "# x_ic = torch.linspace(x_min, x_max, N_ic).view(-1, 1)\n",
    "x_ic = torch.rand(N_ic, 1)*(x_max-x_min)+x_min\n",
    "t_ic = torch.zeros(N_ic, 1)\n",
    "u_ic_target = ic_func(x_ic)\n",
    "\n",
    "# Load observed training data from file\n",
    "params = {'T': 100.0, 'L': 1.0, 'N': 2048, 'data_path': '../../../data/ks_training.npy'}\n",
    "data_obs = load_training_data(params)\n",
    "# Use keys 'x_data', 't_data', 'u_data_target' from the loaded data dictionary.\n",
    "\n",
    "# Bundle all components into a single data dictionary for loss evaluation.\n",
    "data = {\n",
    "    'collocation': (x_f, t_f),\n",
    "    'boundary': (x_left, t_left, x_right, t_right),\n",
    "    'initial': (x_ic, t_ic, u_ic_target),\n",
    "    'data': (data_obs['x_data'], data_obs['t_data'], data_obs['u_data_target'])\n",
    "}\n",
    "\n",
    "# ===================================================\n",
    "# 8. Construct the PINN Model and Trainable Parameter nu\n",
    "# ===================================================\n",
    "\n",
    "# Model: input dimension is 2 (x and t), output dimension is 1.\n",
    "model = NeuralNetwork(num_layers=4, num_neurons=50, input_size=2, output_size=1, activation_function=tanh)\n",
    "print(model)\n",
    "\n",
    "# Unknown parameter nu defined as a trainable parameter.\n",
    "nu = torch.nn.Parameter(torch.tensor([0.5], dtype=torch.float32))\n",
    "\n",
    "# ===================================================\n",
    "# 9. Training Process\n",
    "# ===================================================\n",
    "\n",
    "optimizer = optim.Adam(list(model.parameters()) + [nu], lr=0.001)\n",
    "epochs = 5000\n",
    "\n",
    "loss_history = []\n",
    "residual_loss_history = []\n",
    "boundary_loss_history = []\n",
    "initial_loss_history = []\n",
    "data_loss_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    total_loss, loss_r, loss_b, loss_ic, loss_d = train_loss(model, data, nu)\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    loss_history.append(total_loss.item())\n",
    "    residual_loss_history.append(loss_r.item())\n",
    "    boundary_loss_history.append(loss_b.item())\n",
    "    initial_loss_history.append(loss_ic.item())\n",
    "    data_loss_history.append(loss_d.item())\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}: Total Loss={total_loss.item():.6f}, Residual Loss={loss_r.item():.6f}, \"\n",
    "              f\"Boundary Loss={loss_b.item():.6f}, Initial Loss={loss_ic.item():.6f}, \"\n",
    "              f\"Data Loss={loss_d.item():.6f}, nu={nu.item():.6f}\")\n",
    "\n",
    "print(\"Training complete. Final nu =\", nu.item())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: params['N'] (100) does not match the number of spatial points in the training data (2048). Using data shape.\n",
      "NeuralNetwork(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1-3): 3 x Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 0: Total Loss=4.897291, Residual Loss=0.000009, Boundary Loss=0.000017, Initial Loss=0.551996, Data Loss=4.345269, nu=0.499001\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "88c8be8427f9c9ec"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML4DE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
